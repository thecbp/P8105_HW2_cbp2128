---
title: "R Notebook"
author: "Christian B Pascual"
date: "2018-09-28"
output: github_document
---

# Preliminary Imports

We need to practice our data wrangling and tidying skills in this homework. Thus, we'll bring in `tidyverse` and `readxl` for reading and cleaning data. For Problem 3, we need the `p8105.datasets` package.

```{r lib_import, message = FALSE }
library(tidyverse)
library(readxl)
library(knitr)
library(p8105.datasets)
```

# Problem 1

This problem requires us to bring in and clean the NYC Transit dataset.

```{r P1_load_data, message = FALSE }
raw_nyc_data = read_csv(file = './data/P1/nyc_subway.csv')
```

`raw_nyc_data` certainly needs some cleaning. Each route has its own column, and many of the cells in these route columns are missing. Some stations have multiple rows that only differ on the entrance locations. This information isn't needed for our analyses, so much of the data is redundant. Lastly, the problem doesn't need many of the columns in the raw dataset, so we should remove them.

As we clean the data, we'll address these issues and end up with a tidy data set.

```{r P1_data_cleaning }
tidy_nyc_data = raw_nyc_data %>% 
  janitor::clean_names() %>% 
  gather(data = ., key = route_number, value = route_served, route1:route11, na.rm = TRUE) %>% 
  mutate(can_enter = ifelse(entry == "YES", TRUE, FALSE),
         has_vending = ifelse(vending == "YES", TRUE, FALSE)) %>% 
  select(., station_name, line, route_served, station_latitude, station_longitude, entrance_type, can_enter, has_vending, is_ada = ada) %>% 
  distinct(., station_name, line, route_served, .keep_all = TRUE) %>%
  arrange(., station_name, line) 
```

## The cleaning process

`tidy_nyc_data` is the result of our cleaning process. After fixing up the column names, I used `gather()` to place all the route served information nito a single column so that we get a 1-to-1 correspondence for each station to every route they serve. Gathering the routes removes the multitude of missing values present in the route columns. `entry` and `vending` have been converted into Boolean types and have been renamed to be more descriptive of this nature (the same renaming was done to `ada`). I used `select()` to rearrange the columns to give station name the main focus, while removing extraneous columns not required in the problem. To remove duplicate station rows, I used `distinct()` to make sure that each station-line-route combination is unique.

The ending result keeps the station and line information, and each route has its own row. Other station information such as the station location, entrance type, ADA status has been kept. Customers who may need to figure out if they can even enter a particular station or purchase a Metro Card can use the corresponding Boolean columns.

## From raw to tidy

The original dataset had `r nrow(raw_nyc_data)` rows and `r ncol(raw_nyc_data)` columns, while the processed dataset has been reduced to `r nrow(tidy_nyc_data)` rows and `r ncol(tidy_nyc_data)` columns. Some stations are seen multiple times if they house different lines or serve multiple routes. The data is now in tidy form.

## Problem 1 Data Inquiry Questions

### How many distinct stations are there?

If each station is identified by the station and line, then we can use the `distinct()` function again. A station is stated to be uniquely identifiable by a station-line combination, so we'll make sure that we remove duplicates based on both of these columns.

```{r P1_Q1_answer }
distinct_stations = tidy_nyc_data %>% 
  distinct(., station_name, line, .keep_all = TRUE)
```

`distinct_stations` tells us that there are `r nrow(distinct_stations)` distinct stations.

The later questions also require information from distinct stations, so I've set `.keep_all` to `TRUE` to make sure we still have access to that information.

### How many stations are ADA compliant?

Using the `is_ada` column, we can inquire how many of the stations are ADA compliant. Since we need to look at distinct stations, we can use the same code from above and just count how many rows in `is_ada` are `TRUE`.

`r sum(distinct_stations$is_ada)` distinct stations are ADA compliant.

### What proportion of station entrances / exits without vending allow entrance?

We are asked to calculate the proportion of station entrances that allow entrance, given that they don't have vending. We can calculate this from our `distinct_stations` dataset.

```{r P1_Q3_answer }
# Number of stations with no vending
without_vending = filter(distinct_stations, distinct_stations$has_vending == FALSE)

is_enterable = sum(without_vending$can_enter == TRUE)
```

The proportion of stations without vending that allow entrance is `r is_enterable/nrow(without_vending)`.

### How many distinct stations serve the A train? How many of them ADA compliant?

We want all distinct stations that serve the A line, so we can filter for those results. Of these A-serving trains, we can also calculate how many are ADA compliant.

```{r P1_Q4_answer }
serves_A = filter(distinct_stations, route_served == "A")
```

The number of distinct stations that serve the A train is `r nrow(serves_A)`, and the number of them that are compliant is `r sum(serves_A$is_ada == TRUE)`. Uh oh.

# Problem 2 

This problem asks us to read and clean the Mr. Trash Wheel data set.

![Actual Picture of Me](./me_but_im_mr_trash.jpg)

## Loading Mr. Trash Data

```{r P2_trash_data_loading }
raw_trash_data = read_xlsx('./data/P2/water_wheel.xlsx',
                           sheet = 'Mr. Trash Wheel',
                           range = "A2:N256") # Read in only data, not notes
```

`raw_trash_data` needs some work. The data has some summary rows that give the cumulative monthly amounts after a month finishes, but most of the other rows detail daily trash amounts. There's some other formatting issues we'd like to take care of with the quantitative data (there's no such thing as 6.2 sports balls).

```{r P2_data_tidying }
better_col_names = paste(colnames(raw_trash_data), "collected")
colnames(raw_trash_data) = better_col_names

tidy_trash_data = raw_trash_data %>% 
  janitor::clean_names() %>% 
  filter(., !is.na(.$dumpster_collected)) %>% 
  mutate(sports_balls_int = as.integer(sports_balls_collected)) %>% 
  select(., everything(), -sports_balls_collected)
```

## Loading precipitation data

Next, we need to import and start cleaning the precipitation data from 2016 and 2017. 

```{r P2_precipitation_data_loading }
precip_2016 = read_xlsx('./data/P2/water_wheel.xlsx',
                        sheet = '2016 Precipitation',
                        range = 'A2:B14')

precip_2017 = read_xlsx('./data/P2/water_wheel.xlsx',
                        sheet = '2017 Precipitation',
                        range = 'A2:B14')
```

These data sets originally came in two separate sheets but contain the same kind of data, so they will need to be combined later on. Like the trash data, there are some rows that need to be removed since some data are missing in the 2017 precipitation data. These data look alike, but we need to add a year column before we can append these datasets together. In the case of the 2017 data, we need to remove the `NA` rows. 

```{r}
tidy_precip_2016 = precip_2016 %>% 
  janitor::clean_names() %>% 
  mutate(year = 2016,
         month = month.name) %>% 
  select(., year, month, total)

tidy_precip_2017 = precip_2017 %>% 
  janitor::clean_names() %>% 
  mutate(year = 2017,
         month = month.name) %>% 
  filter(., !is.na(.$total)) %>% 
  select(., year, month, total)

tidy_precipitation = bind_rows(tidy_precip_2016, tidy_precip_2017)
```

The end result is a tidy dataset that even Mr. Trash Wheel can be proud of.

## Discussion of the datasets

The result is daily information on what kinds of trash Mr. Trash Wheel is picking up. We can see in the `polystyrene` and `sports_balls_int` columns that the machine is always picking up various items that shouldn't be in the river.

In the beginning the 2016 precipitation dataset had `r nrow(precip_2016)` rows, and the 2017 dataset had `r nrow(precip_2017)` rows. After all the data cleaning and manipulation, the resulting preciptation data is `r nrow(tidy_precipitation)` rows and has more descriptive `year` and `month` descriptions. The `total` column gives us the amount of precipitation on a given `year` and `month`.

## Problem 2 Data Inquiry Questions

### What was the total precipitation in 2017? 

First, we need to properly select the 2017 data from the dataset.

```{r P2_question_1_answer_1 }
just_2017_precip = tidy_precipitation %>% 
  filter(., year == 2017) %>% 
  select(., total)
```

`tidy_precipitation` tells us that `r sum(just_2017_precip$total)` inches of rain fell in 2017, given the data that was present for that year.

### What was the median number of sports balls in a dumpster in 2016?

As above, we need to isolate the data for 2016 in `tidy_trash_data`.

```{r P2_question_1_answer_2 }
just_2016_sports_balls = tidy_trash_data %>% 
  filter(., year_collected == 2016) %>% 
  select(., sports_balls_int)
```

`tidy_trash_data` tells us that the median amount of sports balls in a dumpster in 2016 was `r median(just_2016_sports_balls$sports_balls_int)`.

# Problem 3

In order to load the BRFSS dataset without actually downloading it onto our computers, we needed to install the `p8105.datasets` package. From there, we can get the BRFSS data.

```{r P3_data_loading }
raw_BRFSS = p8105.datasets::brfss_smart2010
```

The BRFSS data is quite large and there's some formatting that needs to be done to get it into a more inquiry-able form. We need to exclude some columns for "class, topic, question, sample size, and everything from lower confidence limit to GeoLocation". Response questions need to be formatted, and we need a new proportion variable based on these responses.

```{r P3_data_tidying }
tidy_BRFSS = raw_BRFSS %>% 
  filter(., Topic == "Overall Health") %>% 
  select(., -Class, -Topic, -Question, -Sample_Size, -(Confidence_limit_Low:GeoLocation)) %>% 
  spread(., key = Response, value = Data_value) %>% 
  mutate(excellently_very_good = Excellent + `Very good`) %>% 
  janitor::clean_names()
```

We went from `r nrow(raw_BRFSS)` rows and `r ncol(raw_BRFSS)` columns in the raw BRFSS dataset to `r nrow(tidy_BRFSS)` rows and `r ncol(tidy_BRFSS)` columns in the cleaned dataset. Each `response_value` now has its own column with the `data_value` column populating each cell. 

## Problem 3 Data Inquiry Questions

With this dataset, we can answer Problem 3's questions.

### How many unique locations are included in the dataset? 

There are `r length(unique(tidy_BRFSS$locationdesc))` unique locations included in the dataset.

### Is every state represented? What state is observed the most?

The dataset tells us that there are `r length(unique(tidy_BRFSS$locationabbr))` present, indicating all 50 states including the District of Columbia (state abbreviation: DC).

To get the state with the most observations, we'll need another tibble containing the frequency counts of each state, which we can get with `count()`.

```{r P3_state_counts }
state_counts = tidy_BRFSS %>% 
  count(., vars = locationabbr) %>% 
  arrange(., -n)
```

After getting the counts and rearranging in descending order, we get that `r slice(state_counts, 1)$vars` is the state with the most observations at `r slice(state_counts, 1)$n` rows.

### In 2002, what is the median of the “Excellent” response value?

The median "Excellent" response proportion in the dataset is `r median(tidy_BRFSS$excellent, na.rm = TRUE)`%.

### Make a histogram of “Excellent” response values in the year 2002.

```{r P3_2002_excellent_plot, warning = FALSE }
BRFSS_2002 = filter(tidy_BRFSS, year == 2002)
ggplot(BRFSS_2002, aes(x = excellent)) + 
  geom_histogram(binwidth = 1, color = "black", fill = "white") + 
  labs(
    title = "Histogram of Excellent response values in BRFSS Data, 2002",
    x = "Proportion of 'excellent' response values",
    y = "Frequency count"
  )
```

The histogram tells us that the proportion of "Excellent" responses in the 2002 data hovers around 21-23%, with some counties skewing the data to the right.

### Make a scatterplot showing the proportion of “Excellent” response values in New York County and Queens County from 2002 to 2010.

First, we'll simplify the dataset to fit our plotting needs better.

```{r P3_ny_vs_queens }
county_data = tidy_BRFSS %>% 
  filter(., .$locationdesc == "NY - New York County" | .$locationdesc == "NY - Queens County") %>% 
  filter(., .$year >= 2002 & .$year <= 2010) %>% 
  select(., year, county = locationdesc, excellent)
```

With the proper data selected, we can produce the scatterplot easily.

```{r P3_ny_vs_queens_plot }
ggplot(county_data, aes(x = year, y = excellent)) +
  geom_point(aes(color = county)) +
  labs(
    title = "Scatterplot of Excellent response values of New York and Queens County, 2002 - 2010",
    x = "Year",
    y = "Proportion of 'excellent' response values"
  )
```

The scatterplot tells us that there are consistently a greater proportion of "Excellent" responses in New York County than in Queens County from 2002 to 2010. 

Most calculations and graphs have also been validated in Excel since it's not easy to get a feel for the entire BRFSS dataset, even after paring it down.
